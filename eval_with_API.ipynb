{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7445c85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fears for T N pension after talks Unions repre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP) ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP) ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP) AP...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Fears for T N pension after talks Unions repre...      2\n",
       "1  The Race is On: Second Private Team Sets Launc...      3\n",
       "2  Ky. Company Wins Grant to Study Peptides (AP) ...      3\n",
       "3  Prediction Unit Helps Forecast Wildfires (AP) ...      3\n",
       "4  Calif. Aims to Limit Farm-Related Smog (AP) AP...      3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get training dataset\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "df_train.head()\n",
    "\n",
    "# Get testing dataset\n",
    "df_test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5801d10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced Training Set Label Distribution:\n",
      "label\n",
      "0    1000\n",
      "1    1000\n",
      "2    1000\n",
      "3    1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Imbalanced Training Set Label Distribution:\n",
      "label\n",
      "0    2000\n",
      "1     200\n",
      "2     200\n",
      "3     200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Sample out an balanced training data\n",
    "n_rows_per_class = 1000\n",
    "balanced_dfs = []\n",
    "\n",
    "for label in sorted(df_train[\"label\"].unique()):\n",
    "    class_samples = df_train[df_train[\"label\"] == label]\n",
    "    balanced_dfs.append(class_samples.sample(n_rows_per_class, random_state=42))\n",
    "\n",
    "balanced_data = pd.concat(balanced_dfs)\n",
    "\n",
    "# Sample out an imbalanced training data (Assume label 0 as the majority class)\n",
    "n_majority = 2000\n",
    "n_minority = 200\n",
    "\n",
    "imbalanced_dfs = []\n",
    "label_0_class = df_train[df_train[\"label\"] == 0]\n",
    "imbalanced_dfs.append(label_0_class.sample(n_majority, random_state=42))\n",
    "\n",
    "for label in sorted(df_train[\"label\"].unique())[1:]:\n",
    "    class_samples = df_train[df_train['label'] == label]\n",
    "    imbalanced_dfs.append(class_samples.sample(n_minority, random_state=42))\n",
    "\n",
    "imbalanced_data = pd.concat(imbalanced_dfs)\n",
    "# Shuffle the imbalanced dataset to mix the classes\n",
    "imbalanced_data = imbalanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nBalanced Training Set Label Distribution:\")\n",
    "print(balanced_data['label'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nImbalanced Training Set Label Distribution:\")\n",
    "print(imbalanced_data['label'].value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "654b1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small testing set\n",
    "n_rows_per_class = 1000\n",
    "test_balanced_dfs = []\n",
    "for label in sorted(df_test[\"label\"].unique()):\n",
    "    test_samples = df_test[df_test[\"label\"] == label]\n",
    "    test_balanced_dfs.append(test_samples.sample(n_rows_per_class, random_state=42))\n",
    "\n",
    "testing_set = pd.concat(test_balanced_dfs)\n",
    "testing_set = testing_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5017f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets to parquet for later use\n",
    "balanced_data.to_parquet('Data/ag_news_train_balanced.parquet')\n",
    "imbalanced_data.to_parquet('Data/ag_news_train_imbalanced.parquet')\n",
    "testing_set.to_parquet('Data/ag_news_test_small.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c33036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to build the prompt strings on both balanced and imbalanced\n",
    "label_map = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}\n",
    "\n",
    "def build_shots_prompt(train_df, shots=None, imbalanced_ratio=None):\n",
    "    prompt_lines = [\"Classify the following text into one of these categories: World, Sports, Business, Sci/Tech\", \n",
    "                   \"\", \n",
    "                   \"IMPORTANT: Respond with ONLY the category name, nothing else.\",\n",
    "                   \"\",\n",
    "                   \"Examples:\"]\n",
    "    \n",
    "    if shots:\n",
    "        # Build a balanced prompt\n",
    "        for label in sorted(train_df['label'].unique()):\n",
    "            class_samples = train_df[train_df['label'] == label].sample(shots, random_state=42)\n",
    "            for _, row in class_samples.iterrows():\n",
    "                prompt_lines.append(f\"Text: {row['text']}\")\n",
    "                prompt_lines.append(f\"Category: {label_map[row['label']]}\")\n",
    "                prompt_lines.append(\"\")  # Add a blank line between examples\n",
    "                \n",
    "    elif imbalanced_ratio:\n",
    "        # Build an imbalanced prompt based on the provided ratios\n",
    "        for label, n_shots in imbalanced_ratio.items():\n",
    "            class_samples = train_df[train_df['label'] == label].sample(n_shots, random_state=42)\n",
    "            for _, row in class_samples.iterrows():\n",
    "                prompt_lines.append(f\"Text: {row['text']}\")\n",
    "                prompt_lines.append(f\"Category: {label_map[row['label']]}\")\n",
    "                prompt_lines.append(\"\")  # Add a blank line between examples\n",
    "    else:\n",
    "        raise ValueError(\"Must provide either 'shots_per_class' or 'imbalanced_ratios'\")\n",
    "        \n",
    "    # Join all lines into a single string\n",
    "    prompt_str = \"\\n\".join(prompt_lines)\n",
    "    return prompt_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ef34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text, examples_prompt, model_name, api_key):\n",
    "    full_prompt = f\"{examples_prompt}\\n\\nNow classify this new text:\\nText: {text}\\nCategory:\"\n",
    "    payload = {\n",
    "        \"model\": model_name, \n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.0,  \n",
    "        \"max_tokens\": 10     \n",
    "    }\n",
    "\n",
    "    # The headers required by Open Router\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            data=json.dumps(payload)\n",
    "        )\n",
    "        response.raise_for_status() \n",
    "        \n",
    "        result = response.json()\n",
    "        # Extract the model's response, which is similar to the OpenAI API.\n",
    "        prediction = result['choices'][0]['message']['content'].strip()\n",
    "        pred_lines = prediction.splitlines()[0].lower()\n",
    "        for label in label_map.values():\n",
    "            if label.lower() in pred_lines:\n",
    "                return label\n",
    "        return prediction.strip()\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error for text '{text[:50]}...': {e}\")\n",
    "        time.sleep(5)  \n",
    "        return \"Error\"\n",
    "    except KeyError as e:\n",
    "        print(f\"Could not parse response for text '{text[:50]}...': {e}. Response: {result}\")\n",
    "        return \"Error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5e21680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "--- EVALUATING meta-llama/llama-4-maverick:free ---\n",
      "Running with Balanced prompt\n",
      "Sample: True='Business', Raw='Business', Cleaned='Business'\n",
      "Sample: True='Sports', Raw='Sports', Cleaned='Sports'\n",
      "Sample: True='Business', Raw='Business', Cleaned='Business'\n",
      "Request error for text 'U.S. Treasuries Inch Up, Await Data  LONDON (Reute...': 429 Client Error: Too Many Requests for url: https://openrouter.ai/api/v1/chat/completions\n",
      "Unrecognized prediction: 'error' -> Mapping to 'Unknown'\n",
      "Request error for text 'Toyota reports a silicon carbide breakthrough Move...': 429 Client Error: Too Many Requests for url: https://openrouter.ai/api/v1/chat/completions\n",
      "Unrecognized prediction: 'error' -> Mapping to 'Unknown'\n",
      "Sample: True='Sci/Tech', Raw='Error', Cleaned='Unknown'\n",
      "Unique predictions: {'Sci/Tech', 'Unknown', 'World', 'Business', 'Sports'}\n",
      "Unrecognized prediction: 'unknown' -> Mapping to 'Unknown'\n",
      "Unrecognized prediction: 'unknown' -> Mapping to 'Unknown'\n",
      "    Balanced Prompt Macro-F1: 0.7475\n",
      "Running with Imbalanced prompt\n",
      "Sample: True='Business', Raw='Business', Cleaned='Business'\n",
      "Sample: True='Sports', Raw='Sports', Cleaned='Sports'\n",
      "Sample: True='Business', Raw='Business', Cleaned='Business'\n",
      "Request error for text 'U.S. Treasuries Inch Up, Await Data  LONDON (Reute...': 502 Server Error: Bad Gateway for url: https://openrouter.ai/api/v1/chat/completions\n",
      "Unrecognized prediction: 'error' -> Mapping to 'Unknown'\n",
      "Sample: True='Sci/Tech', Raw='Sci/Tech', Cleaned='Sci/Tech'\n",
      "Unique predictions: {'Sci/Tech', 'Unknown', 'World', 'Business', 'Sports'}\n",
      "Unrecognized prediction: 'unknown' -> Mapping to 'Unknown'\n",
      "    Imbalanced Prompt Macro-F1: 0.7705\n",
      "Evaluation done\n",
      "\n",
      "--- Balanced Prompt Results ---\n",
      "Macro-F1: 0.7475\n",
      "Prediction distribution: {'Sports': 6, 'Business': 5, 'World': 3, 'Sci/Tech': 4, 'Unknown': 2}\n",
      "\n",
      "--- Imbalanced Prompt Results ---\n",
      "Macro-F1: 0.7705\n",
      "Prediction distribution: {'Sports': 6, 'Business': 7, 'World': 2, 'Sci/Tech': 4, 'Unknown': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "\n",
    "model = \"meta-llama/llama-4-maverick:free\"\n",
    "\n",
    "# Define expected labels\n",
    "expected_labels = list(label_map.values())\n",
    "print(f\"Expected labels: {expected_labels}\")\n",
    "\n",
    "balanced_prompt = build_shots_prompt(balanced_data, shots=2)\n",
    "imbalanced_prompt = build_shots_prompt(balanced_data, imbalanced_ratio={0: 4, 1: 1, 2: 1, 3: 1})\n",
    "\n",
    "print(f\"--- EVALUATING {model} ---\")\n",
    "model_res = {}\n",
    "\n",
    "def clean_prediction(pred, expected_labels):\n",
    "    if not pred:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Convert to lowercase and remove extra spaces\n",
    "    pred = pred.lower().strip()\n",
    "    \n",
    "    \n",
    "    pred = re.sub(r'[^\\w\\s]', '', pred)  \n",
    "    pred = re.sub(r'\\b(category|is|the|a|an)\\b', '', pred)  \n",
    "    pred = pred.strip()\n",
    "    \n",
    "    \n",
    "    for label in expected_labels:\n",
    "        if label.lower() in pred or pred in label.lower():\n",
    "            return label\n",
    "    \n",
    "   \n",
    "    variation_map = {\n",
    "        'sport': 'Sports',\n",
    "        'sci': 'Sci/Tech',\n",
    "        'technology': 'Sci/Tech',\n",
    "        'tech': 'Sci/Tech',\n",
    "        'businesses': 'Business',\n",
    "        'world news': 'World'\n",
    "    }\n",
    "    \n",
    "    for variation, correct_label in variation_map.items():\n",
    "        if variation in pred:\n",
    "            return correct_label\n",
    "    \n",
    "    \n",
    "    print(f\"Unrecognized prediction: '{pred}' -> Mapping to 'Unknown'\")\n",
    "    return \"Unknown\"\n",
    "\n",
    "for prompt_name, prompt in [(\"Balanced\", balanced_prompt), (\"Imbalanced\", imbalanced_prompt)]:\n",
    "    print(f\"Running with {prompt_name} prompt\")\n",
    "    \n",
    "    y_pred_arr = []\n",
    "    y_true_arr = []\n",
    "    \n",
    "    n_rows = 20\n",
    "\n",
    "    for _, row in testing_set.iloc[n_rows:40].iterrows():\n",
    "        y_true = label_map[row[\"label\"]]\n",
    "        raw_pred = classify(\n",
    "            text=row[\"text\"],\n",
    "            examples_prompt=prompt,\n",
    "            model_name=model,\n",
    "            api_key=API_KEY\n",
    "        )\n",
    "        \n",
    "       \n",
    "        cleaned_pred = clean_prediction(raw_pred, expected_labels)\n",
    "        \n",
    "        y_true_arr.append(y_true)\n",
    "        y_pred_arr.append(cleaned_pred)\n",
    "        \n",
    "        \n",
    "        if len(y_pred_arr) % 5 == 0:\n",
    "            print(f\"Sample: True='{y_true}', Raw='{raw_pred}', Cleaned='{cleaned_pred}'\")\n",
    "\n",
    "    \n",
    "    unique_preds = set(y_pred_arr)\n",
    "    print(f\"Unique predictions: {unique_preds}\")\n",
    "    \n",
    "    \n",
    "    report = classification_report(y_true_arr, y_pred_arr, \n",
    "                                    labels=expected_labels,\n",
    "                                    target_names=expected_labels,\n",
    "                                    output_dict=True, \n",
    "                                    zero_division=0)\n",
    "    \n",
    "    model_res[prompt_name] = {\n",
    "        'predictions': y_pred_arr,\n",
    "        'true_labels': y_true_arr,\n",
    "        'raw_predictions': [clean_prediction(p, expected_labels) for p in y_pred_arr],\n",
    "        'classification_report': report,\n",
    "        'macro_f1': report['macro avg']['f1-score']\n",
    "    }\n",
    "\n",
    "    print(f\"    {prompt_name} Prompt Macro-F1: {report['macro avg']['f1-score']:.4f}\")\n",
    "    \n",
    "   \n",
    "    time.sleep(60)  \n",
    "\n",
    "print(\"Evaluation done\")\n",
    "\n",
    "# Print detailed results\n",
    "for prompt_name in model_res:\n",
    "    print(f\"\\n--- {prompt_name} Prompt Results ---\")\n",
    "    print(f\"Macro-F1: {model_res[prompt_name]['macro_f1']:.4f}\")\n",
    "    \n",
    "    \n",
    "    pred_counts = {}\n",
    "    for pred in model_res[prompt_name]['predictions']:\n",
    "        pred_counts[pred] = pred_counts.get(pred, 0) + 1\n",
    "    print(f\"Prediction distribution: {pred_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7dcd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
